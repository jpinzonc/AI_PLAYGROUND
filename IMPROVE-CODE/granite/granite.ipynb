{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IBN GRANITE MODELS\n",
    "https://github.com/ibm-granite/granite-code-models\n",
    "\n",
    "\n",
    "To use any of our models, pick an appropriate model_path from:\n",
    "ibm-granite/granite-3b-code-base\n",
    "ibm-granite/granite-3b-code-instruct\n",
    "ibm-granite/granite-8b-code-base\n",
    "ibm-granite/granite-8b-code-instruct\n",
    "ibm-granite/granite-20b-code-base\n",
    "ibm-granite/granite-20b-code-instruct\n",
    "ibm-granite/granite-34b-code-base\n",
    "ibm-granite/granite-34b-code-instruct\n",
    "'''\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "device = \"cuda\" # or \"cpu\"\n",
    "model_path = \"ibm-granite/granite-3b-code-instruct\" # pick anyone from above list\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# Config added from repo to handle larger text inputs\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "# config.max_seq_len = 4096\n",
    "# config.max_answer_len= 1024\n",
    "config.max_new_tokens = 250\n",
    "config.max_length = 250\n",
    "\n",
    "# drop device_map if running on CPU\n",
    "model_code = AutoModelForCausalLM.from_pretrained(model_path\n",
    "                                            #  , device_map=device\n",
    "                                             , config = config)\n",
    "model_code.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_information(model, question, device):\n",
    "    input_tokens = tokenizer(question, return_tensors=\"pt\")\n",
    "\n",
    "    # transfer tokenized inputs to the device\n",
    "    # for i in input_tokens:\n",
    "    #     input_tokens[i] = input_tokens[i].to(device)\n",
    "\n",
    "    # generate output tokens\n",
    "    output = model.generate(**input_tokens)\n",
    "    # decode output tokens into text\n",
    "    output = tokenizer.batch_decode(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"def generate():\"\n",
    "test_out = get_information(model_code, test_text, device)\n",
    "# loop over the batch to print, in this example the batch size is 1\n",
    "for i in test_out:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '''what does this function do {}'''\n",
    "example1 = base.format('''\n",
    "even= []\n",
    "uneven=[]\n",
    "for i in INPUT:\n",
    "  if i %2 ==0:\n",
    "    even.append(i)\n",
    "  else:\n",
    "    uneven.append(i)\n",
    "org = uneven + even\n",
    "print(org)''')\n",
    "# loop over the batch to print, in this example the batch size is 1\n",
    "for i in get_information(model_code, example1, device):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2 = base.format('''\n",
    "final = []\n",
    "for item in INPUT:\n",
    "  if item %2 ==0:\n",
    "    final.insert(len(final)+1, item)\n",
    "  else:\n",
    "    final.insert(0, item)\n",
    "print(final)''')\n",
    "# loop over the batch to print, in this example the batch size is 1\n",
    "for i in get_information(model_code, example2, device):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example3 = base.format('''import pandas as pd\n",
    "def create_df():\n",
    "    data = {'state': ['Ohio','Ohio','Ohio','Nevada','Nevada'],\n",
    "           'year': [2000,2001,2002,2001,2002],\n",
    "           'pop': [1.5,1.7,3.6,2.4,2.9]}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "### We'll create three dataframes for an example\n",
    "for i in range(3):\n",
    "    exec(f'df_{i} = create_df()')''')\n",
    "for i in get_information(model_code, example3, device):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morgage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama on LOCAL\n",
    "\n",
    "## Example 1  - LLAMA3 using Ollama\n",
    "\n",
    "The project is about caracterizing transactions and generating a dashboard. \n",
    "\n",
    "- The original author __Thu Vu data analytics__ publications/data can be found here: \n",
    "\n",
    "https://www.youtube.com/watch?v=h_GTxRFYETY\n",
    "\n",
    "https://github.com/thu-vu92/local-llms-analyse-finance\n",
    "\n",
    "https://ollama.com/\n",
    "\n",
    "\n",
    "This example includes: \n",
    "\n",
    "- LLM Categorization of transactions \n",
    "\n",
    "- DASHBOARD on Ploty plot \n",
    "\n",
    "### Getting Ollama: \n",
    "\n",
    "1. Go to ollama.ai \n",
    "\n",
    "2. Click download \n",
    "\n",
    "### Installing Ollama\n",
    "\n",
    "1. Double clink on ollama downloaded file and follow instructions \n",
    "\n",
    "### Get models\n",
    "\n",
    "1. on Terminal: \n",
    "\n",
    "    ollama pull llama3\n",
    "\n",
    "3. Generate own model (Optional), create a file using nano: \n",
    "\n",
    "```nano expense_analyzer```\n",
    "\n",
    "Enter this information in the file and save it: \n",
    "\n",
    "        FROM llama3\n",
    "\n",
    "        PARAMETER temperature 0.8\n",
    "\n",
    "        SYSTEM You are a financial assistan, you help classify expenses and income from bank trnsactions\n",
    "\n",
    "3. Run this code\n",
    "\n",
    "```ollama create expense_analyzer_llama3 -f ./expense_analyzer```\n",
    "\n",
    "4. To remov the model: \n",
    "\n",
    "```ollama rm <model_name>```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the list of expenses with categories:\\n\\nSpotify AB by Adyen - Entertainment, Beta Boulders Ams Amsterdam Nld - Sports, ISS Catering Services De Meern - Food, Vishandel Sier AMSTELVEEN - Food, Ministerie van Justitie en Veiligheid - Government, Etos AMSTERDAM NLD - Health/Beauty, Bistro Bar Amsterdam - Food/Dining'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "# Generate the model\n",
    "llm3 = Ollama(model ='llama3')\n",
    "\n",
    "# This is testing the connetion to the model works: \n",
    "llm3.invoke(\"Can you add an appropriate category next to each of the following expenses. Respond with a list of categories separated by commas. For example, Spotify AB by Adyen - \\\n",
    "            Entertainment, Beta Boulders Ams Amsterdam Nld - Sports, etc.: \\\n",
    "            ISS Catering Services De Meern, Vishandel Sier AMSTELVEEN, Ministerie van Justitie en Veiligheid, Etos AMSTERDAM NLD, Bistro Bar Amsterdam\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date        Name / Description Expense/Income  Amount (EUR)\n",
      "0  2023-12-30           Belastingdienst        Expense          9.96\n",
      "1  2023-12-30               Tesco Breda        Expense         17.53\n",
      "2  2023-12-30   Monthly Appartment Rent        Expense        451.00\n",
      "3  2023-12-30  Vishandel Sier Amsterdam        Expense         12.46\n",
      "4  2023-12-29         Selling Paintings         Income         13.63\n",
      "\n",
      "Number of unique transactions:  23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Tesco Breda', 'Monthly Appartment Rent',\n",
       "       'Vishandel Sier Amsterdam', 'Selling Paintings',\n",
       "       'Spotify Ab By Adyen', 'Tk Maxx Amsterdam Da', 'Consulting',\n",
       "       'Aidsfonds', 'Tls Bv Inz Ov-Chipkaart'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading the dataset with the transactions \n",
    "df = pd.read_csv('transactions_2022_2023.csv', low_memory = False )\n",
    "print(df.head())\n",
    "# Get unique transactions in the Name / Description column\n",
    "unique_transactions = df[\"Name / Description\"].unique()\n",
    "print('\\nNumber of unique transactions: ', len(unique_transactions))\n",
    "unique_transactions[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 23]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get index list\n",
    "# https://stackoverflow.com/questions/47518609/for-loop-range-and-interval-how-to-include-last-step\n",
    "def hop(start, stop, step):\n",
    "    for i in range(start, stop, step):\n",
    "        yield i\n",
    "    yield stop\n",
    "# Spit the index into groups of 30\n",
    "index_list = list(hop(0, len(unique_transactions), 30))\n",
    "index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseChecks(data=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output validation using pydanthic\n",
    "from pydantic import BaseModel, field_validator\n",
    "from typing import List\n",
    "\n",
    "# Validate response format - check if it actually contains hyphen (\"-\")\n",
    "class ResponseChecks(BaseModel):\n",
    "    data: List[str]\n",
    "\n",
    "    @field_validator(\"data\")\n",
    "    def check(cls, value):\n",
    "        for item in value:\n",
    "            if len(item) > 0:\n",
    "                assert \"-\" in item, \"String does not contain hyphen.\"\n",
    "\n",
    "# Test validation\n",
    "ResponseChecks(data=['Hello - World', 'Hello - there!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_transactions(transaction_names, llm):\n",
    "    response = llm.invoke(\"Can you add an appropriate category to the following expenses. For example: Spotify AB by Adyen - Entertainment, Beta Boulders Ams Amsterdam Nld - Sport, etc.. Categories should be less than 4 words. \" + transaction_names)\n",
    "    response = response.split('\\n')\n",
    "    # Keep only the lines in between blank lines (removing the explaination lines at the beginning and end of the response)\n",
    "    blank_indexes = [index for index in range(len(response)) if response[index] == '']\n",
    "    if len(blank_indexes) == 1:\n",
    "        response = response[(blank_indexes[0] + 1):]\n",
    "    else:\n",
    "        response = response[(blank_indexes[0] + 1) : blank_indexes[1]]\n",
    "    # Print response and validate if it is in the correct format\n",
    "    print(response)\n",
    "    #### Was getting an error and include this step to prevent it to happen\n",
    "    response = [s.replace('* ', '') for s in response] \n",
    "    # return response\n",
    "    ResponseChecks(data = response)    \n",
    "    # Put in dataframe\n",
    "    categories_df = pd.DataFrame({'Transaction vs category': response})\n",
    "    return categories_df\n",
    "    #### Keeping the below inside the function did not work. I move this outside after copying the dataframe into a new one. \n",
    "    # categories_df[['Transaction', 'Category']] = categories_df['Transaction vs category'].str.split(' - ', expand=True)\n",
    "    # return categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the function\n",
    "# r = categorize_transactions('ISS Catering Services De Meern, Vishandel Sier AMSTELVEEN, Etos AMSTERDAM NLD, Bistro Bar Amsterdam', llm3)\n",
    "# r[['Transaction', 'Category']] = r['Transaction vs category'].str.split(' - ', expand=True)\n",
    "# r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Intialise the categories_df_all dataframe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m categories_df_all \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      3\u001b[0m max_tries \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Loop through the index_list\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Intialise the categories_df_all dataframe\n",
    "categories_df_all = pd.DataFrame()\n",
    "max_tries = 7\n",
    "\n",
    "# Loop through the index_list\n",
    "for i in range(0, len(index_list)-1):\n",
    "    transaction_names = unique_transactions[index_list[i]:index_list[i+1]]\n",
    "    transaction_names = ','.join(transaction_names)\n",
    "\n",
    "    # Try and validate output, if it fails, try again for max_tries=7 times\n",
    "    for j in range(1, max_tries):\n",
    "        try:\n",
    "            categories_df = categorize_transactions(transaction_names, llm3)\n",
    "            categories_df_all = pd.concat([categories_df_all, categories_df], ignore_index=True)\n",
    "            \n",
    "        except:\n",
    "            if j < max_tries:\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception(f\"Cannot categorise transactions indexes {i} to {i+1}.\")\n",
    "        break\n",
    "categories_df_all[['Transaction', 'Category']] = categories_df_all['Transaction vs category'].str.split(' - ', expand=True)\n",
    "print(categories_df_all.head(5))\n",
    "# Get unique categories in categories_df_all\n",
    "unique_categories = categories_df_all[\"Category\"].unique()\n",
    "unique_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manual formating of some categories\n",
    "# Drop NA values\n",
    "categories_df_all = categories_df_all.dropna()\n",
    "# If category contains \"Food\", then categorise as \"Food and Drinks\"\n",
    "categories_df_all.loc[categories_df_all['Category'].str.contains(\"Food\"), 'Category'] = \"Food and Drinks\"\n",
    "# If category contains \"Clothing\", then categorise as \"Clothing\"\n",
    "categories_df_all.loc[categories_df_all['Category'].str.contains(\"Clothing\"), 'Category'] = \"Clothing\"\n",
    "# If category contains \"Services\", then categorise as \"Services\"\n",
    "categories_df_all.loc[categories_df_all['Category'].str.contains(\"Services\"), 'Category'] = \"Services\"\n",
    "# If category contains \"Health\" or \"Wellness\", then categorise as \"Health and Wellness\"\n",
    "categories_df_all.loc[categories_df_all['Category'].str.contains(\"Health|Wellness\"), 'Category'] = \"Health and Wellness\"\n",
    "# If category contains \"Sport\", then categorise as \"Sport\n",
    "#  and Fitness\"\n",
    "categories_df_all.loc[categories_df_all['Category'].str.contains(\"Sport\"), 'Category'] = \"Sport and Fitness\"\n",
    "# If category contains \"Travel\", then categorise as \"Travel\"\n",
    "categories_df_all.loc[categories_df_all['Category'].str.contains(\"Travel\"), 'Category'] = \"Travel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the numbering eg \"1. \" from Transaction column\n",
    "# Needs regex = True in the replace function\n",
    "cat_copy = categories_df_all.copy()\n",
    "cat_copy['Transaction'] = cat_copy['Transaction'].str.replace(r'\\d+\\.\\s+', '', regex=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from platform import python_version\n",
    "\n",
    "# if python_version() == '3.10.9':\n",
    "#     print(python_version())\n",
    "#     cat_copy['Transaction'] = cat_copy['Transaction'].str.replace(r'\\d+\\.\\s+', '', regex=True)  \n",
    "# else: \n",
    "#     print(python_version())\n",
    "#     # Removing the numbers in the category\n",
    "#     cat_copy.loc[:,'Transact'] = cat_copy['Transaction'].str.split('.', expand=True)[1]\n",
    "#     cat_copy.loc[:,'Transact'] = cat_copy.Transact.str.strip()\n",
    "#     cat_copy.drop(['Transaction', 'Transaction vs category'], axis = 1, inplace = True)\n",
    "#     cat_copy.rename(columns = {'Transact': 'Transaction'}, inplace = True)\n",
    "#     cat_copy.Transaction.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Merge the categories_df_all with the transactions_2022_2023.csv dataframe (df)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransactions_2022_2023.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName / Description\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpotify\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName / Description\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpotify Ab By Adyen\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmerge(cat_copy, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName / Description\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransaction\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Merge the categories_df_all with the transactions_2022_2023.csv dataframe (df)\n",
    "df = pd.read_csv(\"transactions_2022_2023.csv\")\n",
    "df.loc[df['Name / Description'].str.contains(\"Spotify\"), 'Name / Description'] = \"Spotify Ab By Adyen\"\n",
    "df = df.merge(cat_copy, left_on='Name / Description', right_on='Transaction', how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorizefilenm = \"transactions_2022_2023_categorized.csv\"\n",
    "df.to_csv(categorizefilenm, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing libraries for dashboard. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import panel as pn\n",
    "\n",
    "if df.empty:    \n",
    "    # Read transactions_2022_2023_categorized.csv\n",
    "    df = pd.read_csv(categorizefilenm = \"transactions_2022_2023_categorized.csv\", low_memory = False)\n",
    "\n",
    "# Add year and month columns\n",
    "df['Year'] = pd.to_datetime(df['Date']).dt.year\n",
    "df['Month'] = pd.to_datetime(df['Date']).dt.month\n",
    "df['Month Name'] = pd.to_datetime(df['Date']).dt.strftime(\"%b\")\n",
    "# Remove \"Transaction\" and \"Transaction vs category\" columns\n",
    "df = df.drop(columns=['Transaction'])\n",
    "# For Income rows, assign Name / Description to Category\n",
    "df['Category'] = np.where(df['Expense/Income'] == 'Income', df['Name / Description'], df['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pie_chart(df, year, label):\n",
    "    # Filter the dataset for expense transactions\n",
    "    sub_df = df[(df['Expense/Income'] == label) & (df['Year'] == year)]\n",
    "\n",
    "    color_scale = px.colors.qualitative.Set2\n",
    "    \n",
    "    pie_fig = px.pie(sub_df, values='Amount (EUR)', names='Category', color_discrete_sequence = color_scale)\n",
    "    pie_fig.update_traces(textposition='inside', direction ='clockwise', hole=0.3, textinfo=\"label+percent\")\n",
    "\n",
    "    total_expense = df[(df['Expense/Income'] == 'Expense') & (df['Year'] == year)]['Amount (EUR)'].sum() \n",
    "    total_income = df[(df['Expense/Income'] == 'Income') & (df['Year'] == year)]['Amount (EUR)'].sum()\n",
    "    \n",
    "    if label == 'Expense':\n",
    "        total_text = \"€ \" + str(round(total_expense))\n",
    "\n",
    "        # Saving rate:\n",
    "        saving_rate = round((total_income - total_expense)/total_income*100)\n",
    "        saving_rate_text = \": Saving rate \" + str(saving_rate) + \"%\"\n",
    "    else:\n",
    "        saving_rate_text = \"\"\n",
    "        total_text = \"€ \" + str(round(total_income))\n",
    "\n",
    "    pie_fig.update_layout(uniformtext_minsize=10, \n",
    "                        uniformtext_mode='hide',\n",
    "                        title=dict(text=label+\" Breakdown \" + str(year) + saving_rate_text),\n",
    "                        # Add annotations in the center of the donut.\n",
    "                        annotations=[\n",
    "                            dict(\n",
    "                                text=total_text, \n",
    "                                # Square unit grid starting at bottom left of page\n",
    "                                x=0.5, y=0.5, font_size=12,\n",
    "                                # Hide the arrow that points to the [x,y] coordinate\n",
    "                                showarrow=False\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "    return pie_fig\n",
    "income_pie_fig_2022 = make_pie_chart(df, 2022, 'Income')\n",
    "income_pie_fig_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_monthly_bar_chart(df, year, label):\n",
    "    df = df[(df['Expense/Income'] == label) & (df['Year'] == year)]\n",
    "    total_by_month = (df.groupby(['Month', 'Month Name'])['Amount (EUR)'].sum()\n",
    "                        .to_frame()\n",
    "                        .reset_index()\n",
    "                        .sort_values(by='Month')  \n",
    "                        .reset_index(drop=True))\n",
    "    if label == \"Income\":\n",
    "        color_scale = px.colors.sequential.YlGn\n",
    "    if label == \"Expense\":\n",
    "        color_scale = px.colors.sequential.OrRd\n",
    "    \n",
    "    bar_fig = px.bar(total_by_month, x='Month Name', y='Amount (EUR)', text_auto='.2s', title=label+\" per month\", color='Amount (EUR)', color_continuous_scale=color_scale)\n",
    "    # bar_fig.update_traces(marker_color='lightslategrey')\n",
    "    \n",
    "    return bar_fig\n",
    "income_monthly_2022 = make_monthly_bar_chart(df, 2022, 'Income')\n",
    "income_monthly_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie charts\n",
    "income_pie_fig_2022 = make_pie_chart(df, 2022, 'Income')\n",
    "expense_pie_fig_2022 = make_pie_chart(df, 2022, 'Expense')  \n",
    "income_pie_fig_2023 = make_pie_chart(df, 2023, 'Income')\n",
    "expense_pie_fig_2023 = make_pie_chart(df, 2023, 'Expense')\n",
    "\n",
    "# Bar charts\n",
    "income_monthly_2022 = make_monthly_bar_chart(df, 2022, 'Income')\n",
    "expense_monthly_2022 = make_monthly_bar_chart(df, 2022, 'Expense')\n",
    "income_monthly_2023 = make_monthly_bar_chart(df, 2023, 'Income')\n",
    "expense_monthly_2023 = make_monthly_bar_chart(df, 2023, 'Expense')\n",
    "\n",
    "# Create tabs\n",
    "tabs = pn.Tabs(\n",
    "                        ('2022', pn.Column(pn.Row(income_pie_fig_2022, expense_pie_fig_2022),\n",
    "                                                pn.Row(income_monthly_2022, expense_monthly_2022))),\n",
    "                        ('2023', pn.Column(pn.Row(income_pie_fig_2023, expense_pie_fig_2023),\n",
    "                                                pn.Row(income_monthly_2023, expense_monthly_2023))\n",
    "                        )\n",
    "                )\n",
    "tabs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard template\n",
    "template = pn.template.FastListTemplate(\n",
    "    title='Personal Finance Dashboard',\n",
    "    sidebar=[pn.pane.Markdown(\"# Income Expense analysis\"), \n",
    "             pn.pane.Markdown(\"Overview of income and expense based on my bank transactions. Categories are obtained using local LLMs.\"),\n",
    "             pn.pane.PNG(\"picture.png\", sizing_mode=\"scale_both\")\n",
    "             ],\n",
    "    main=[pn.Row(pn.Column(pn.Row(tabs)\n",
    "                           )\n",
    "                ),\n",
    "                ],\n",
    "    # accent_base_color=\"#88d8b0\",\n",
    "    header_background=\"#c0b9dd\",\n",
    ")\n",
    "\n",
    "template.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "<h1 style='color:red;'>CHANGE TO Python 3.10.9</h1>\n",
    "\n",
    "## Example 2  - LLAMA3 using LancChain\n",
    "\n",
    "The project is about caracterizing transactions and generating a dashboard. \n",
    "\n",
    "- The original author __LanngChain__ publications/data can be found here: \n",
    "\n",
    "https://www.youtube.com/watch?v=-ROS6gfYIts\n",
    "\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/llms/ollama/\n",
    "\n",
    "\n",
    "- For tracing it requires an account from: \n",
    "\n",
    "https://smith.langchain.com/\n",
    "\n",
    "- For web search: \n",
    "\n",
    "https://app.tavily.com/\n",
    "\n",
    "You may need to run this: \n",
    "\n",
    "!pip install -U langchain-nom?ic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python gpt4all langchain-text-splitters langchain-chroma sentence-transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('../', 'secret')))\n",
    "from secret_info import lanchain_trace_api, tavily_api\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = lanchain_trace_api\n",
    "os.environ['TAVILY_API_KEY'] = tavily_api\n",
    "### LLM - Name of the model\n",
    "local_llm = \"llama3\"\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgepinzon/opt/anaconda3/envs/py310/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Users/jorgepinzon/opt/anaconda3/envs/py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Index\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Python 3.12 results in this error: ImportError: Could not import chromadb python package. Please install it with `pip install chromadb`.\n",
    "# Use python 3.10.9\n",
    "vectorstore = Chroma.from_documents(documents=doc_splits, \n",
    "                                    embedding=embedder, \n",
    "                                    persist_directory='.'\n",
    "                                    )\n",
    "# vectorstore.persist()\n",
    "retriever = vectorstore.as_retriever()\n",
    "# # In order to work needs Python 3.10\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=doc_splits,\n",
    "#     collection_name=\"rag-chroma\",\n",
    "# #    embedding = GPT4AllEmbeddings(),\n",
    "# #    persist_directory= False\n",
    "# )\n",
    "# retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "                                        of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "                                        grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "                                        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "                                        Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "                                            <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "                                        Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "                                        Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "                                        \"\"\",\n",
    "                        input_variables=[\"question\", \"document\"],\n",
    "                        )\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "                                    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "                                    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "                                    Question: {question} \n",
    "                                    Context: {context} \n",
    "                                    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "                        input_variables=[\"question\", \"document\"],\n",
    "                        )\n",
    "# llm = ChatOllama(model=local_llm, temperature=0)\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "# Run\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader\n",
    "# LLM\n",
    "# llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "# Prompt\n",
    "prompt = PromptTemplate(template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether \n",
    "                                    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate \n",
    "                                    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "                                    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "                                    Here are the facts:\n",
    "                                    \\n ------- \\n\n",
    "                                    {documents} \n",
    "                                    \\n ------- \\n\n",
    "                                    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "                        input_variables=[\"generation\", \"documents\"],\n",
    "                        )\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer Grader\n",
    "# LLM\n",
    "# llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "# Prompt\n",
    "prompt = PromptTemplate(template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an \n",
    "                                    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "                                    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "                                        <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "                                    \\n ------- \\n\n",
    "                                    {generation} \n",
    "                                    \\n ------- \\n\n",
    "                                    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "                        input_variables=[\"generation\", \"question\"],\n",
    "                        )\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Router\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# LLM\n",
    "# llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "prompt = PromptTemplate(template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "                                    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \n",
    "                                    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \n",
    "                                    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \n",
    "                                    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "                                    no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "                        input_variables=[\"question\"],\n",
    "                        )\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "question = \"llm agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "### State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "### Nodes\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "### Conditional edge\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    print(source)\n",
    "    print(source[\"datasource\"])\n",
    "    if source[\"datasource\"] == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source[\"datasource\"] == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "    \n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "### Conditional edge\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "# Test\n",
    "inputs = {\"question\": \"Who are the Bears expected to draft first in the NFL draft?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morgage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

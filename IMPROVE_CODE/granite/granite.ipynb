{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM GRANITE MODELS\n",
    "\n",
    "- https://github.com/ibm-granite/granite-code-models\n",
    "\n",
    "To use any of our models, pick an appropriate model_path from:\n",
    "\n",
    "        ibm-granite/granite-3b-code-base\n",
    "\n",
    "        ibm-granite/granite-3b-code-instruct\n",
    "\n",
    "        ibm-granite/granite-8b-code-base\n",
    "\n",
    "        ibm-granite/granite-8b-code-instruct\n",
    "\n",
    "        ibm-granite/granite-20b-code-base\n",
    "\n",
    "        ibm-granite/granite-20b-code-instruct\n",
    "\n",
    "        ibm-granite/granite-34b-code-base\n",
    "\n",
    "        ibm-granite/granite-34b-code-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, GenerationConfig\n",
    "# model_path = \"distilbert/distilgpt2\" # pick anyone from above list\n",
    "model_path = \"ibm-granite/granite-3b-code-instruct\" # pick anyone from above list\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# change input text as desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa1b9e1144342dfbd6aa80e3450e711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ibm-granite/granite-3b-code-instruct were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading the model - this will download the model to your local env. \n",
    "# https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using chat \n",
    "# https://huggingface.co/ibm-granite/granite-8b-code-instruct\n",
    "\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"Write a code to find the maximum value in a list of numbers.\" },\n",
    "]\n",
    "chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# tokenize the text\n",
    "input_tokens = tokenizer(chat, return_tensors=\"pt\")\n",
    "# transfer tokenized inputs to the device\n",
    "for i in input_tokens:\n",
    "    input_tokens[i] = input_tokens[i].to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output tokens\n",
    "output = model.generate(**input_tokens, max_new_tokens=100)\n",
    "# decode output tokens into text\n",
    "output = tokenizer.batch_decode(output)\n",
    "# loop over the batch to print, in this example the batch size is 1\n",
    "for i in output:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With no chat\n",
    "# https://huggingface.co/docs/transformers/main/en/main_classes/text_generation\n",
    "input_text = '''Please generate the doc string section for this python function: def sum(a, b): retunrn a+b'''\n",
    "# tokenize the text\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "for i in input_tokens:\n",
    "    input_tokens[i] = input_tokens[i].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**input_tokens, max_new_tokens = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode output tokens into text\n",
    "output = tokenizer.batch_decode(output)\n",
    "\n",
    "# loop over the batch to print, in this example the batch size is 1\n",
    "for i in output:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original from Github above.\n",
    " \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "device = \"cpu\" #\"cuda\" # or \"cpu\"\n",
    "model_path = \"ibm-granite/granite-3b-code-instruct\" # pick anyone from above list\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# Config added from repo to handle larger text inputs\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "# config.max_seq_len = 4096\n",
    "# config.max_answer_len= 1024\n",
    "config.max_new_tokens = 250\n",
    "config.max_length = 250\n",
    "\n",
    "# drop device_map if running on CPU\n",
    "model_code = AutoModelForCausalLM.from_pretrained(model_path\n",
    "                                            #  , device_map=device\n",
    "                                             , config = config)\n",
    "model_code.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change input text as desired\n",
    "input_text = '''Please generate the doc string section for this python function: def sum(a, b): retunrn a+b'''\n",
    "# tokenize the text\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer tokenized inputs to the device\n",
    "for i in input_tokens:\n",
    "    input_tokens[i] = input_tokens[i].to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output tokens\n",
    "output = model_code.generate(**input_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode output tokens into text\n",
    "output = tokenizer.batch_decode(output)\n",
    "\n",
    "# loop over the batch to print, in this example the batch size is 1\n",
    "for i in output:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_information(model, question, device):\n",
    "    input_tokens = tokenizer(question, return_tensors=\"pt\")\n",
    "\n",
    "    # transfer tokenized inputs to the device\n",
    "    for i in input_tokens:\n",
    "         input_tokens[i] = input_tokens[i].to(device)\n",
    "\n",
    "    # generate output tokens\n",
    "    output = model.generate(**input_tokens)\n",
    "    # decode output tokens into text\n",
    "    output = tokenizer.batch_decode(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = '''Generte the docstring for the following puython function: \n",
    "def generate():\n",
    "a = 1+1\n",
    "return a'''\n",
    "test_out = get_information(model_code, test_text, device)\n",
    "# loop over the batch to print, in this example the batch size is 1\n",
    "for i in test_out:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '''what does this python function do {}'''\n",
    "example1 = base.format('''\n",
    "even= []\n",
    "uneven=[]\n",
    "for i in INPUT:\n",
    "  if i %2 ==0:\n",
    "    even.append(i)\n",
    "  else:\n",
    "    uneven.append(i)\n",
    "org = uneven + even\n",
    "print(org)''')\n",
    "# loop over the batch to print, in this example the batch size is 1\n",
    "for i in get_information(model_code, example1, device):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headers= {\n",
    "        \"Authorization\": f\"Bearer {hug_face}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "def hug_face_api_call(question, headers, API_URL=None):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3\" if API_URL == None else API_URL\n",
    "\n",
    "    payload_template =  {\n",
    "                    \"inputs\":question,\n",
    "                    \"parameters\" : {\n",
    "                                    \"max_length\": 40,\n",
    "                                    }\n",
    "                    }  \n",
    "    try: \n",
    "        response = requests.post(API_URL, headers=headers, json=payload_template)\n",
    "        answer = response.json()[0]['generated_text']\n",
    "        return response, answer\n",
    "    except Exception as E: \n",
    "        print(str(E))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2 = base.format('''\n",
    "final = []\n",
    "for item in INPUT:\n",
    "  if item %2 ==0:\n",
    "    final.insert(len(final)+1, item)\n",
    "  else:\n",
    "    final.insert(0, item)\n",
    "print(final)''')\n",
    "# loop over the batch to print, in this example the batch size is 1\n",
    "for i in get_information(model_code, example2, device):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '''what does this python function do {}'''\n",
    "example3 = base.format('''import pandas as pd\n",
    "def create_df():\n",
    "    data = {'state': ['Ohio','Ohio','Ohio','Nevada','Nevada'],\n",
    "           'year': [2000,2001,2002,2001,2002],\n",
    "           'pop': [1.5,1.7,3.6,2.4,2.9]}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "### We'll create three dataframes for an example\n",
    "for i in range(3):\n",
    "    exec(f'df_{i} = create_df()')''')\n",
    "for i in get_information(model_code, example3, device):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" # or \"cpu\"\n",
    "model_path = \"ibm-granite/granite-3b-code-base\" # pick anyone from above list\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# drop device_map if running on CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)#, device_map=device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# change input text as desired\n",
    "input_text = \"def generate():\"\n",
    "# tokenize the text\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# transfer tokenized inputs to the device\n",
    "for i in input_tokens:\n",
    "    input_tokens[i] = input_tokens[i]#.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output tokens\n",
    "output = model.generate(**input_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode output tokens into text\n",
    "output = tokenizer.batch_decode(output)\n",
    "\n",
    "# loop over the batch to print, in this example the batch size is 1\n",
    "for i in output:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morgage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
